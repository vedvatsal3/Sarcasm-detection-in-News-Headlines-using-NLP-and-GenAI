{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxV8SCXCsc6W",
        "outputId": "bee39fc1-9748-4b5e-c08c-71b776130c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=289a89eb716e613972771b539a364886c9bede229e3215df22f86195669b32ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n",
            "Requirement already satisfied: afinn in /usr/local/lib/python3.10/dist-packages (0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install afinn\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from afinn import Afinn\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and initialize the sentiment analyzer\n",
        "data = pd.read_csv(\"deadpan_sarcasm.csv\")"
      ],
      "metadata": {
        "id": "xhsIeb-ataBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "afinn = Afinn()\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFDv3kxmuwnm",
        "outputId": "e6173764-6b8a-4b18-d012-457adc079747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to count positive and negative words\n",
        "def count_pos_words(text):\n",
        "    words = text.split()\n",
        "    pos_count = sum([afinn.score(word) > 0 for word in words])\n",
        "    return pos_count\n",
        "\n",
        "def count_neg_words(text):\n",
        "    words = text.split()\n",
        "    neg_count = sum([afinn.score(word) < 0 for word in words])\n",
        "    return neg_count\n",
        "\n",
        "# Add positive and negative word count features\n",
        "data['positive_word_count'] = data['headline'].apply(count_pos_words)\n",
        "data['negative_word_count'] = data['headline'].apply(count_neg_words)"
      ],
      "metadata": {
        "id": "embmLHR7u4oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to calculate sentiment score\n",
        "def get_sentiment_score(text):\n",
        "    sentiment_score = sia.polarity_scores(text)[\"compound\"]\n",
        "    return sentiment_score\n",
        "\n",
        "# Calculate sentiment score\n",
        "data['sentiment_score'] = data['headline'].apply(get_sentiment_score)"
      ],
      "metadata": {
        "id": "ENP-MEAVvBVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the features (headlines) and labels (sarcasm types) including the additional features\n",
        "X = data[['headline', 'positive_word_count', 'negative_word_count', 'sentiment_score']]\n",
        "y = data['ChatGPT_Alltypes']"
      ],
      "metadata": {
        "id": "zSKZpv6NvMvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the sarcasm types to a binary label (1 for deadpan, 0 for others)\n",
        "y = y.map(lambda x: 1 if x == 'Deadpan' else 0)\n"
      ],
      "metadata": {
        "id": "7rp7ZeLavUnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Geg6IgvqvVQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer to convert text into numerical features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train['headline'])\n",
        "X_test_vec = vectorizer.transform(X_test['headline'])"
      ],
      "metadata": {
        "id": "eq8wxgY2vXki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the NumPy array into a DataFrame\n",
        "X_train_array = pd.DataFrame(X_train_vec.toarray())\n",
        "X_test_array = pd.DataFrame(X_test_vec.toarray())"
      ],
      "metadata": {
        "id": "hgy0JW23vZAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the additional features with the TF-IDF features\n",
        "X_train_features = pd.concat([X_train_array, X_train.drop('headline', axis=1)], axis=1)\n",
        "X_test_features = pd.concat([X_test_array, X_test.drop('headline', axis=1)], axis=1)"
      ],
      "metadata": {
        "id": "Zgln_Gn2vawX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert column names to strings\n",
        "X_train_features.columns = X_train_features.columns.astype(str)\n",
        "X_test_features.columns = X_test_features.columns.astype(str)"
      ],
      "metadata": {
        "id": "DuZCkspdvcEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of RandomUnderSampler\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_features, y_train)\n",
        "X_test_resampled, y_test_resampled = rus.fit_resample(X_test_features, y_test)\n"
      ],
      "metadata": {
        "id": "29ODKptivm2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with an imputer and logistic regression\n",
        "pipeline = make_pipeline(SimpleImputer(strategy='mean'), LogisticRegression())\n",
        "\n",
        "# Fit the model using the pipeline\n",
        "pipeline.fit(X_train_resampled, y_train_resampled)"
      ],
      "metadata": {
        "id": "UkgcoanuvnUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the pipeline\n",
        "y_pred = pipeline.predict(X_test_resampled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test_resampled, y_pred))"
      ],
      "metadata": {
        "id": "_qI3Ng7VvxJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with an imputer and Support Vector Classification (SVM)\n",
        "pipeline = make_pipeline(SimpleImputer(strategy='mean'), SVC(probability=True))\n",
        "\n",
        "# Fit the model using the pipeline\n",
        "pipeline.fit(X_train_resampled, y_train_resampled)\n"
      ],
      "metadata": {
        "id": "g98qWEbPRqx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with an imputer and Random Forest Classifier\n",
        "pipeline = make_pipeline(SimpleImputer(strategy='mean'), RandomForestClassifier())\n",
        "\n",
        "# Fit the model using the pipeline\n",
        "pipeline.fit(X_train_resampled, y_train_resampled)"
      ],
      "metadata": {
        "id": "fPi7M2eSRrSP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}